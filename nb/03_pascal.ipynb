{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_pascal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1it_ofrfea5o8yPviR3riXAEaFj3g7epc",
      "authorship_tag": "ABX9TyPj1DuxJ6Gm/bQGxXIdnmek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benihime91/pytorch_retinanet/blob/master/nb/03_pascal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6qQWP6i0TCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What GPU do we have ?\n",
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbvhLAldCPZx",
        "colab_type": "text"
      },
      "source": [
        "### **Standard Imports & Setup:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPv4W85JpQZS",
        "colab_type": "text"
      },
      "source": [
        "**Setup:**\n",
        "\n",
        "\n",
        "The cell below ensures that Google Collab doesn't disconnect to inactivity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DimpLe6l0ZV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensure colab doesn't disconnect\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzKR4Mas0bts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies\n",
        "! pip install pytorch-lightning wandb  --quiet\n",
        "! pip install git+https://github.com/albumentations-team/albumentations --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4dFVI694str",
        "colab_type": "text"
      },
      "source": [
        "**Mount `GoogleDrive` & extract the Data:**  \n",
        "\n",
        "\n",
        "The data is stored in the following paths:\n",
        "- train_data : `/content/drive/My Drive/Pascal 2007 Data/pascal_voc_2007_train_val.zip`.\n",
        "- test_data : `/content/drive/My Drive/Pascal 2007 Data/pascal_voc_2007_test_val.zip`.\n",
        "\n",
        "These folders should contain images in a folder `/Images` and annotations in a folder `/Annotations`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYYgjADS4jix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM345So_0db0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab the Data\n",
        "! unzip -qq /content/drive/My\\ Drive/Pascal\\ 2007\\ Data/pascal_voc_2007_test.zip\n",
        "! unzip -qq /content/drive/My\\ Drive/Pascal\\ 2007\\ Data/pascal_voc_2007_train_val.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVy453Kj0ezz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the RetinaNet Repo:\n",
        "! git clone https://github.com/benihime91/pytorch_retinanet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKMiEqQnZiP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use wandb to track experiments : Comment this if not using wandb logger\n",
        "! wandb login 'a74f67fd5fae293e301ea8b6710ee0241f595a63'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Aip8AapZJZ",
        "colab_type": "text"
      },
      "source": [
        "**Imports:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXn7Cf1B0gUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "from typing import *\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNIGOfxd07r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# PyTorch Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# PyTorchLightning imports\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import (EarlyStopping, ModelCheckpoint, LearningRateLogger,)\n",
        "\n",
        "# Import some usefull utilities from the RetinaNet Repo:\n",
        "from pytorch_retinanet import DetectionDataset, Visualizer, Retinanet\n",
        "from pytorch_retinanet.src.utils.coco_utils import CocoEvaluator, get_coco_api_from_dataset\n",
        "from pytorch_retinanet.src.utils.general_utils import collate_fn\n",
        "\n",
        "\n",
        "pl.seed_everything(123) # change this seed number to get different results\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeKEAo1wpwhf",
        "colab_type": "text"
      },
      "source": [
        "### **Sanity Check:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO0MlXYB__-A",
        "colab_type": "text"
      },
      "source": [
        "**Load in the Preprocessed Data:**\n",
        "\n",
        "Run this [notebook](https://github.com/benihime91/pytorch_retinanet/blob/master/nbs/01_preprocess_pascal.ipynb) to process the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muY8VDpMR3Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path to the converted DataFrames\n",
        "\n",
        "# NOTE: Update these path names from to the paths where your train , validation, test_data and \n",
        "# image_targets (in pickle format) is saved !\n",
        "trn_csv_dir = '/content/drive/My Drive/Pascal 2007 Data/trn_data.csv' # path to the train csv file\n",
        "val_csv_dir = '/content/drive/My Drive/Pascal 2007 Data/val_data.csv' # path to the valid csv file\n",
        "tst_csv_dir = '/content/drive/My Drive/Pascal 2007 Data/tst_data.csv' # path to the test csv file\n",
        "label_dir = \"/content/drive/My Drive/Pascal 2007 Data/names.pkl\" # path to label dictionary saved in pickle format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxec0sHSSOkM",
        "colab_type": "text"
      },
      "source": [
        "**Let's cross check the data to make sure everything is all right.. At the same time let's visualize some examples from the data ...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEL2Ad5f2nag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load in the DataFrames\n",
        "trn_df = pd.read_csv(trn_csv_dir)\n",
        "val_df = pd.read_csv(val_csv_dir)\n",
        "tst_df = pd.read_csv(tst_csv_dir)\n",
        "\n",
        "# Load in the Label Dict\n",
        "label_dict = pickle.load(open(label_dir, \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdjPwwQdA8Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print out some information about the Data:\n",
        "print('Num examples in train dataset :', len(trn_df.filename.unique()))\n",
        "print('Num examples in valid dataset :', len(val_df.filename.unique()))\n",
        "print('Num examples in test dataset  :', len(tst_df.filename.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Dl4LWQ6xAt",
        "colab_type": "text"
      },
      "source": [
        "**Train data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R9EZZH9-rYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_df.head() # train dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGRPLhc561cE",
        "colab_type": "text"
      },
      "source": [
        "**Validation Data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g9OjRgQZuB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_df.head() # validation dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KJDGg2F65XS",
        "colab_type": "text"
      },
      "source": [
        "**Test Data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHK-ClX6ZwOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tst_df.head() # test dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVYyU994675f",
        "colab_type": "text"
      },
      "source": [
        "**Label Dictionary (Dicitionary containing the Labels)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEgZt63rZzx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict # a dictionary which stores the mapping of target_labels to class_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70LoSYIsTk7t",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate `Visualizer` to display images with `bboxes`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLQ9dP3J29Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the visualizer\n",
        "viz = Visualizer(class_names=label_dict)\n",
        "\n",
        "def display_random_image(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Fn to display a random Image with bounding boxes drawn above it \n",
        "    from given pandas.Dataframe\n",
        "    \"\"\"\n",
        "    n = np.random.randint(0, len(df))\n",
        "    fname = df[\"filename\"][n]\n",
        "    boxes = df.loc[df[\"filename\"] == fname][[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n",
        "    labels = df.loc[df[\"filename\"] == fname][\"labels\"].values\n",
        "    \n",
        "    viz.draw_bboxes(fname, boxes=boxes, classes=labels, figsize=(10, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6NZeG8D7DnC",
        "colab_type": "text"
      },
      "source": [
        "**View Random Images from the Dataset :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWzs5PpH3CEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display random Image from the train set\n",
        "display_random_image(trn_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2-YWqtF-gf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display random Image from the validation set\n",
        "display_random_image(val_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9wMRVsd-kUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display random Image from the Test Dataset\n",
        "display_random_image(tst_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzB9LE3cqAka",
        "colab_type": "text"
      },
      "source": [
        "### **Image Transformations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6liUHrhAihCa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Instantiate `transforms`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV_zkqs_3GnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tfms() -> Dict[str, A.Compose]:\n",
        "    \"\"\"\n",
        "    Returns a dictionary contatining albumentations \n",
        "    transformations for train,valid,test datasets.\n",
        "    \"\"\"\n",
        "    # train transformations : [Modify this to add Transformations to train dataset] \n",
        "    trn_tfms = [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ToFloat(max_value=255.0, always_apply=True), # range the pixel values between [0, 1]\n",
        "        ToTensorV2(always_apply=True),\n",
        "    ]\n",
        "\n",
        "    # validation transformations : [Transformations to the validation dataset]\n",
        "    val_tfms = [\n",
        "        A.ToFloat(max_value=255.0, always_apply=True),\n",
        "        ToTensorV2(always_apply=True),\n",
        "    ]\n",
        "\n",
        "    # test transformations : [Transformations to the test dataset]\n",
        "    tst_tfms = [\n",
        "        A.ToFloat(max_value=255.0, always_apply=True),\n",
        "        ToTensorV2(always_apply=True),                \n",
        "    ]\n",
        "\n",
        "    # transforms dictionary :\n",
        "    transforms = {\n",
        "        \"train\": A.Compose(trn_tfms, bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]),),\n",
        "        \"valid\": A.Compose(val_tfms, bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]),),\n",
        "        \"test\" : A.Compose(tst_tfms, bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]),),\n",
        "    }\n",
        "    \n",
        "    return transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmNPs4gqqF9-",
        "colab_type": "text"
      },
      "source": [
        "### **Lightning Class:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OZIC6bN6VQZ",
        "colab_type": "text"
      },
      "source": [
        "In this `notebook` we will use `PyTorchLightning` to train, validate and evaluate our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXi9HfkO7ppn",
        "colab_type": "text"
      },
      "source": [
        "**Create `pl.LightningModule` instance :** \n",
        "\n",
        "\n",
        "Here we will instantiate a `pl.LightningModule` instance.The `LightningModule ` holds all the core research ingredients :\n",
        "- The model\n",
        "- The optimizers\n",
        "- The train/ val/ test steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_bF1xrk-eUQ",
        "colab_type": "text"
      },
      "source": [
        "**Lightning class:**  \n",
        "\n",
        "To use pytorch-lightning, we need to define a main class, which has the following parts:\n",
        "\n",
        "- `hparams`- This is optional parameter, but it better to use it - it is a dictionary with hyperparameters;\n",
        "- `forward method` - making predictions with the model. The model itself can be defined outside this class;\n",
        "- `prepare data` - preparing datasets;\n",
        "- `train_dataloader`, `val_dataloader`, `test_dataloader` - these methods should return the relevant dataloaders;\n",
        "- `configure_optimizers` - should return lists of optimizers and schedulers;\n",
        "- `training_step` - define what happend inside the train loop;\n",
        "- `validation_step` - define what happend inside the validation loop;\n",
        "- `validation_epoch_end` - define what happend at epoch end;\n",
        "- `test_step` - define what happend inside the test loop;\n",
        "- `test_epoch_end` - define what happend at epoch end;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4LG_Z-8qfK",
        "colab_type": "text"
      },
      "source": [
        "**In our specific case the `hparams` `Union[Dict, argparse.Namespace]` should contain the following fields:**\n",
        "\n",
        "- `optimizer` `(torch.optim.Optimizer)` - Optimizer for the model\n",
        "- `scheduler` `(Union[torch.optim.lr_scheduler, None])` - Scheduler for the `optimizer`, if no `scheduler` is used set `scheduler` to None.\n",
        "- `trn_df` `(pd.DataFrame)` - train dataframe\n",
        "- `trn_bs` `(int)`-train batch_size\n",
        "- `val_df` `(pd.DataFrame)` - validation dataframe\n",
        "- `val_bs` `(int)`-validation batch_size\n",
        "- `test_df` `(pd.DataFrame)` - test dataframe\n",
        "- `test_bs` `(int)`- test batch_size\n",
        "- `iou_types` `(List)` - this parameter is used for evaluation using `COCO API` set it to `[\"bbox\"]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tw0PB184LEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create pl.LightningModule instance\n",
        "class DetectionModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self, model: nn.Module, hparams: Union[Dict, argparse.Namespace]\n",
        "    ) -> None:\n",
        "        super(DetectionModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.hparams = hparams\n",
        "\n",
        "    def num_batches(self) -> List:\n",
        "        \"\"\"\n",
        "        Returns a list containing the number of batches in train, \n",
        "        val & test dataloaders.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            len(self.train_dataloader()),\n",
        "            len(self.val_dataloader()),\n",
        "            len(self.test_dataloader()),\n",
        "        ]\n",
        "\n",
        "    ##################################################################\n",
        "    ############## Configure Optimizer & Schedulers ##################\n",
        "    ##################################################################\n",
        "    def configure_optimizers(self, *args, **kwargs):\n",
        "        \"instatiates optimizer & scheduler(s)\"\n",
        "        # optimizer\n",
        "        optimizer = self.hparams.optimizer\n",
        "        # scheduler\n",
        "        scheduler = self.hparams.scheduler\n",
        "\n",
        "        if scheduler is not None:\n",
        "            return [optimizer], [scheduler]\n",
        "        else:\n",
        "            return [optimizer]\n",
        "\n",
        "    ##################################################################\n",
        "    ########################## preprare data #########################\n",
        "    ##################################################################\n",
        "    def prepare_data(self, stage=None):\n",
        "        \"\"\"\n",
        "        load in the transformation & reads in the data from given paths.\n",
        "        \"\"\"\n",
        "        # Instantiate Transforms:\n",
        "        self.tfms = get_tfms()\n",
        "        # Load in the DataFrames\n",
        "        self.trn_df = pd.read_csv(self.hparams.trn_df)  # train dataframe\n",
        "        self.val_df = pd.read_csv(self.hparams.val_df)  # valid dataframe\n",
        "        self.test_df = pd.read_csv(self.hparams.test_df)  # test dataframe\n",
        "\n",
        "    ##################################################################\n",
        "    ############# Forward Pass of the Model ##########################\n",
        "    ##################################################################\n",
        "    def forward(self, xb, *args, **kwargs):\n",
        "        \"forward step\"\n",
        "        return self.model(xb)\n",
        "\n",
        "    ##################################################################\n",
        "    ########################### Trainining ###########################\n",
        "    ##################################################################\n",
        "    def train_dataloader(self, *args, **kwargs):\n",
        "        \"instantiate train dataloader\"\n",
        "        # instantiate the trian dataset\n",
        "        train_ds = DetectionDataset(self.trn_df, self.tfms[\"train\"])\n",
        "        # load in the dataloader\n",
        "        trn_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=self.hparams.trn_bs,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "        return trn_dl\n",
        "\n",
        "    def training_step(self, batch, batch_idx, *args, **kwargs):\n",
        "        \"one training step\"\n",
        "        images, targets, _ = batch  # unpack the one batch from the DataLoader\n",
        "        targets = [{k: v for k, v in t.items()} for t in targets]  # Unpack the Targets\n",
        "        loss_dict = self.model(\n",
        "            images, targets\n",
        "        )  # Calculate Losses {regression_loss , classification_loss}\n",
        "        losses = sum(loss for loss in loss_dict.values())  # Calculate Total Loss\n",
        "        return {\"loss\": losses, \"log\": loss_dict, \"progress_bar\": loss_dict}\n",
        "\n",
        "    ##################################################################\n",
        "    ###################### Validation ################################\n",
        "    ##################################################################\n",
        "    def val_dataloader(self, *args, **kwargs):\n",
        "        \"instatiate validation dataloader\"\n",
        "        # instantiate the validaiton dataset\n",
        "        val_ds = DetectionDataset(self.val_df, self.tfms[\"valid\"])\n",
        "        # instantiate dataloader\n",
        "        loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=self.hparams.val_bs,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "        )\n",
        "        \n",
        "        # instantiate coco_api to track metrics\n",
        "        coco = get_coco_api_from_dataset(\n",
        "            loader.dataset\n",
        "        )  # Convert dataset to COCO dataset format: for evaluation\n",
        "        \n",
        "        self.coco_evaluator = CocoEvaluator(\n",
        "            coco, self.hparams.iou_types\n",
        "        )  # Instantiate COCO Evaluator\n",
        "        return loader\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, *args, **kwargs):\n",
        "        \"one validation step\"\n",
        "        images, targets, _ = batch\n",
        "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "        outputs = self.model(images, targets)\n",
        "        res = {\n",
        "            target[\"image_id\"].item(): output\n",
        "            for target, output in zip(targets, outputs)\n",
        "        }\n",
        "        self.coco_evaluator.update(res)\n",
        "        return {}\n",
        "\n",
        "    def validation_epoch_end(self, outputs, *args, **kwargs):\n",
        "        self.coco_evaluator.accumulate()\n",
        "        self.coco_evaluator.summarize()\n",
        "        metric = self.coco_evaluator.coco_eval[\"bbox\"].stats[0]\n",
        "        metric = torch.as_tensor(metric)\n",
        "        logs = {\"valid_mAP\": metric}\n",
        "        return {\n",
        "            \"valid_mAP\": metric,\n",
        "            \"log\": logs,\n",
        "            \"progress_bar\": logs,\n",
        "        }\n",
        "\n",
        "    ##################################################################\n",
        "    ######################## Test ####################################\n",
        "    ##################################################################\n",
        "    def test_dataloader(self, *args, **kwargs):\n",
        "        \"instatiate validation dataloader\"\n",
        "        # instantiate train dataset\n",
        "        test_ds = DetectionDataset(self.test_df, self.tfms[\"test\"])\n",
        "        # instantiate dataloader\n",
        "        loader = DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=self.hparams.test_bs,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "        )\n",
        "        # instantiate coco_api to track metrics\n",
        "        coco = get_coco_api_from_dataset(loader.dataset)\n",
        "        self.test_evaluator = CocoEvaluator(coco, self.hparams.iou_types)\n",
        "        return loader\n",
        "\n",
        "    def test_step(self, batch, batch_idx, *args, **kwargs):\n",
        "        \"one test step\"\n",
        "        images, targets, _ = batch\n",
        "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "        outputs = self.model(images, targets)\n",
        "        res = {\n",
        "            target[\"image_id\"].item(): output\n",
        "            for target, output in zip(targets, outputs)\n",
        "        }\n",
        "        self.test_evaluator.update(res)\n",
        "        return {}\n",
        "\n",
        "    def test_epoch_end(self, outputs, *args, **kwargs):\n",
        "        self.test_evaluator.accumulate()\n",
        "        self.test_evaluator.summarize()\n",
        "        metric = self.test_evaluator.coco_eval[\"bbox\"].stats[0]\n",
        "        metric = torch.as_tensor(metric)\n",
        "        logs = {\"test_mAP\": metric}\n",
        "        \n",
        "        return {\n",
        "            \"test_mAP\": metric,\n",
        "            \"log\": logs,\n",
        "            \"progress_bar\": logs,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJBKeogA9IHC",
        "colab_type": "text"
      },
      "source": [
        "**To train a `LightningModule` we need to use the special `Trainer` class from `PyTorch Lightning`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ2RVdY4qPi1",
        "colab_type": "text"
      },
      "source": [
        "### **Configurations :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkouoIZvRY1u",
        "colab_type": "text"
      },
      "source": [
        "**Configure `LightningModule` and `Trainer` Configurations:** \n",
        "\n",
        "In this part we will create set up our trainining configuration for both the `LightningModule` & the `Trainer`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53urYv-d9ftk",
        "colab_type": "text"
      },
      "source": [
        "**Configuration for `LightningModule`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8geJ9CkJYEK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configurations for `LightningModule`\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Number of Epochs to Train for :\n",
        "# ===================================================================================== #\n",
        "EPOCHS = 55 # set number of epochs to Train for\n",
        "\n",
        "# ==================================================================================== #\n",
        "# Instantiate model\n",
        "# ==================================================================================== #\n",
        "NUM_CLASSES = 20 # Total number of unique Targets\n",
        "BACKBONE = 'resnet50' # backbone for RetinaNet Model\n",
        "# load in the RetinaNet model\n",
        "model = Retinanet(num_classes=NUM_CLASSES, backbone_kind=BACKBONE,)\n",
        "\n",
        "# ==================================================================================== #\n",
        "# Parameters for the Train, Validation & the Test Data\n",
        "# ==================================================================================== #\n",
        "# Train dataset Parametrs:\n",
        "trn_df = trn_csv_dir # path to train_csv file\n",
        "trn_bs = 4 # train batch_size\n",
        "\n",
        "# Valid dataset parametrs:\n",
        "val_df = val_csv_dir # path to validation_csv file\n",
        "val_bs = 32 # validation batch_size\n",
        "\n",
        "# Test dataset parametrs:\n",
        "test_df = tst_csv_dir # path to test_csv file \n",
        "test_bs = 32 # test batch_size\n",
        "\n",
        "# ==================================================================================== #\n",
        "# Optimizer & Scheduler Patameters\n",
        "# ==================================================================================== #\n",
        "LR = 1e-03 # Optimizer learning_rate\n",
        "WEIGHT_DECAY = 0.01  # optimizer weight_decay\n",
        "MOMENTUM = 0.9 # optimzier momentum\n",
        "params = [p for p in model.parameters() if p.requires_grad] # model parameters to train\n",
        "\n",
        "# Instantiate Optimizer\n",
        "optimizer = SGD(params, LR, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM) # Optimizer\n",
        "\n",
        "# Instantiate scheduler\n",
        "# Note: If no scheduler is used set `scheduler` to None\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[32,45], gamma=0.1,) # Scheduler\n",
        "# convert scheduler to lightning format\n",
        "if scheduler is not None:\n",
        "    INTERVAL = \"epoch\" # scheduler interval wether after each 'step' for each 'epoch'\n",
        "    scheduler = {\"scheduler\": scheduler, \"interval\": INTERVAL , \"frequency\": 1,}\n",
        "\n",
        "# ==================================================================================== #\n",
        "# set iou types:\n",
        "# ==================================================================================== #\n",
        "iou_types = ['bbox'] # Required for COCO eval\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Create Dictionary to Store the arguments:\n",
        "# ===================================================================================== #\n",
        "hparams_dict = {\n",
        "    'optimizer'     : optimizer,\n",
        "    'scheduler'     : scheduler,\n",
        "    'trn_df'        : trn_df,\n",
        "    'trn_bs'        : trn_bs,\n",
        "    'val_df'        : val_df,\n",
        "    'val_bs'        : val_bs,\n",
        "    'test_df'       : test_df,\n",
        "    'test_bs'       : test_bs,\n",
        "    'iou_types'     : iou_types,\n",
        "}\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Convert dictionary to `Namespace` it is then easier to load from checkpoint\n",
        "# ===================================================================================== #\n",
        "hparams= argparse.Namespace(**hparams_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHUhONui6nd1",
        "colab_type": "text"
      },
      "source": [
        "**Configuration for `pl.Trainer`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWhfHHj95K4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create configurations for the Trainer Flags :\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Insantiate Logger to Log Training logs :\n",
        "# ===================================================================================== #\n",
        "# Wandb logger: assuming wandb is set-up [Optional]\n",
        "wb_name = f\"{time.strftime('%d-%m-||-%I.%M.%S%-p')}\" # change the run name here\n",
        "wb_p = \"pascal-2007\" # change the project name here\n",
        "wb_logger = WandbLogger(name=wb_name, project=wb_p, anonymous=\"allow\",)\n",
        "\n",
        "# learning_rate logger:\n",
        "lr_logger = LearningRateLogger(logging_interval=\"step\") # we use this to log the learning rate\n",
        "\n",
        "# setup callbacks & loggers into a list\n",
        "# since pl.Trainer expects them to be in a list format\n",
        "logger=[wb_logger]\n",
        "callbacks=[lr_logger]\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Insantiate CheckPoint Callback :\n",
        "# ===================================================================================== #\n",
        "# checkpoint callback will save the top 2 checkpoints\n",
        "# path to the directory where to save the checkpoints\n",
        "fname =f\"/content/drive/My Drive/pascal_checkpoints/{time.strftime('%d-%m-||-%I.%M.%S%-p')}\"\n",
        "os.makedirs(fname, exist_ok=True)\n",
        "checkpoint_callback = ModelCheckpoint(fname, mode=\"max\", monitor=\"valid_mAP\", save_top_k=1,)\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Insantiate EarlyStopping Callback :\n",
        "# ===================================================================================== #\n",
        "early_stop_callback = EarlyStopping(mode=\"max\", monitor=\"valid_mAP\", patience=3,)\n",
        "\n",
        "# ===================================================================================== #\n",
        "#  Additional Trainer Flags:\n",
        "# ===================================================================================== #\n",
        "check_val_every_n_epoch = 10 # Validaiton Check Interval\n",
        "gpus = 1  # gpus to use\n",
        "precision = 16 # precision\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Create Dictionary to the Trainer Flags:\n",
        "# ===================================================================================== #\n",
        "trainer_config_dict = {\n",
        "    'num_sanity_val_steps'   : 0,\n",
        "    'benchmark'              : True, # Set benchmark True to get better performance\n",
        "    'weights_summary'        : None,\n",
        "    'deterministic'          : True,\n",
        "    'terminate_on_nan'       : True,\n",
        "    'logger'                 : logger,\n",
        "    'callbacks'              : callbacks,\n",
        "    'checkpoint_callback'    : checkpoint_callback,\n",
        "    'early_stop_callback'    : early_stop_callback,\n",
        "    'gpus'                   : gpus,\n",
        "    'precision'              : precision,\n",
        "    'max_epochs'             : EPOCHS,\n",
        "    'check_val_every_n_epoch': check_val_every_n_epoch,\n",
        "}\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Convert dictionary to `Namespace`\n",
        "# ===================================================================================== #\n",
        "trainer_config = argparse.Namespace(**trainer_config_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEaIvl5zqlKe",
        "colab_type": "text"
      },
      "source": [
        "## **Train, validate & test:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_BgwVj6tY2",
        "colab_type": "text"
      },
      "source": [
        "**Load trainer & model from the configuration files:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPutRiTL7Als",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load in lightning module from the scpecified hparams\n",
        "retinanet = DetectionModel(model, hparams)\n",
        "\n",
        "# Load in trainer using trainer configs\n",
        "trainer = pl.Trainer.from_argparse_args(trainer_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idVdl1M71K_8",
        "colab_type": "text"
      },
      "source": [
        "**In `PyTorchLightning` training, evaluation, validation , inference ... all is done using the special `Trainer` class.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jvfqjSQvb8D",
        "colab_type": "text"
      },
      "source": [
        "**Fit `RetinaNet` on the train data:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U09Swxi17-ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit Model\n",
        "trainer.fit(retinanet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDaMutnjJ6Zt",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate trained model on the test dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba5L5lkf68vK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model on test dataloader\n",
        "trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys-LX-rDSmDQ",
        "colab_type": "text"
      },
      "source": [
        "**Save model weights:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vem6lnVltfl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set path to where the model should be saved\n",
        "pth = os.path.join(fname, \"model.pt\")\n",
        "torch.save(retinanet.model.state_dict(), pth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuuiySrr1BwC",
        "colab_type": "text"
      },
      "source": [
        "### **Inference:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkj1zjyr2Gsn",
        "colab_type": "text"
      },
      "source": [
        "**Imports & helper functions for inference::**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrrW3u_htbCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Transformations for the Inference Image:\n",
        "infer_tfms = A.Compose([A.ToFloat(max_value=255.0, always_apply=True), ToTensorV2(always_apply=True),])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6SMe60VCkpT",
        "colab_type": "text"
      },
      "source": [
        "**Helper Functions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGBb6WmHGdjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ===================================================================================== #\n",
        "# Get Image Predictions :\n",
        "# ===================================================================================== #\n",
        "@torch.no_grad()\n",
        "def get_preds(path:str, threshold:float=0.5, fname:str=pth, device:Union[str, torch.device]='cpu'):\n",
        "    \"\"\"\n",
        "    Fn to Get predictions on given image\n",
        "\n",
        "    Arguments :\n",
        "    ----------\n",
        "        1. path(str)        : path to the input Image\n",
        "        2. threshold(float) : score threshold to filter predictions. Predicitons\n",
        "                              with score less than threshold are discarded.\n",
        "        3. fname(str)       : path to where model weights are saved. Weights should be a state_dict format\n",
        "        4. device(str or torch.divice): device where to load the model\n",
        "    \"\"\"\n",
        "    ## Parameters to Load in the Model\n",
        "    param_dict = {'num_classes'  : NUM_CLASSES, 'backbone_kind': BACKBONE,}\n",
        "    # Instantiate the model\n",
        "    model = Retinanet(**param_dict)\n",
        "    # Load in the pretrained model weights from weights file\n",
        "    model.load_state_dict(torch.load(fname))\n",
        "    # model to correct device\n",
        "    if model.device != device:\n",
        "        model.to(device)\n",
        "    model.eval() # Set model to eval mode to get bbox predicitons\n",
        "    # Load in the given immage fromm the Image Path\n",
        "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "    # Process the image\n",
        "    img = infer_tfms(image=img)[\"image\"]\n",
        "    img = img.to(device)\n",
        "    # Generate predictions\n",
        "    pred = model([img])\n",
        "    # Unpack predictions\n",
        "    pred_boxes,pred_class,pred_score = pred[0][\"boxes\"],pred[0][\"labels\"],pred[0][\"scores\"]\n",
        "    # Grab the predictions greater than the Given threshold\n",
        "    pred_mask = pred_score > threshold # mask to filter predicitons based on score\n",
        "    # convert predictions to numpy arrays -> List from Tensors\n",
        "    boxes = list(pred_boxes[pred_mask].cpu().numpy())\n",
        "    clas = list(pred_class[pred_mask].cpu().numpy())\n",
        "    scores = list(pred_score[pred_mask].cpu().numpy())\n",
        "    return boxes, clas, scores\n",
        "\n",
        "# ===================================================================================== #\n",
        "# Load Image and Draw predicted bounding box over the Image :\n",
        "# ===================================================================================== #\n",
        "## Fuction to load in the Image , get bbounding-box predictions\n",
        "## and draw the bounding box predictions over the Image.\n",
        "def object_detection_api(img_path:str=None, device:Union[str, torch.device]='cpu', sc_thrs:float=0.5):\n",
        "    \"Draw bbox predictions on given image at img_pth\"\n",
        "    # if Image path is not Given load image path from the user\n",
        "    if img_path is None:\n",
        "        uploaded = files.upload()\n",
        "        img_path = list(uploaded.keys())[0]\n",
        "\n",
        "    bb, cls, sc = get_preds(img_path, sc_thrs, device=device,)\n",
        "    # Load in the Image and draw the predicted bboxes over it\n",
        "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "    viz.draw_bboxes(img, boxes=bb, classes=cls, scores=sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzt4FydpCSC2",
        "colab_type": "text"
      },
      "source": [
        "**Test model predictions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Jy4Drf3xek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.random.randint(0, len(tst_df)) # Grab the Id of a random image fromm the Test Dataset\n",
        "# Detect Objects in the Given Image\n",
        "object_detection_api(device='cuda:0', sc_thrs=0.6, img_path=tst_df[\"filename\"][idx],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8HR9XU6AiSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.random.randint(0, len(tst_df)) # Grab the Id of a random image fromm the Test Dataset\n",
        "# Detect Objects in the Given Image\n",
        "object_detection_api(device='cuda:0', sc_thrs=0.6, img_path=tst_df[\"filename\"][idx],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsuGFdzinAjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.random.randint(0, len(tst_df)) # Grab the Id of a random image fromm the Test Dataset\n",
        "# Detect Objects in the Given Image\n",
        "object_detection_api(device='cuda:0', sc_thrs=0.6, img_path=tst_df[\"filename\"][idx],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7k96DDqnmSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.random.randint(0, len(tst_df)) # Grab the Id of a random image fromm the Test Dataset\n",
        "# Detect Objects in the Given Image\n",
        "object_detection_api(device='cuda:0', sc_thrs=0.6, img_path=tst_df[\"filename\"][idx],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TUAVRKVno6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}