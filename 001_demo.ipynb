{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OqzKnZv0cNSP7Z2SJ_aYkDhcs-iL3DpQ",
      "authorship_tag": "ABX9TyM1tCs7z3NuXSExJ+y4kG3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benihime91/pytorch_retinanet/blob/master/001_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeiJgg30wyo6"
      },
      "source": [
        "# What GPU do we have ?\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwbGnBBawyiS"
      },
      "source": [
        "# Ensure colab doesn't disconnect\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p71emt7TA2Oz"
      },
      "source": [
        "## **Initial-Setup**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZj8vWFgwrzc"
      },
      "source": [
        "# install dependencies\n",
        "! pip install pytorch-lightning wandb omegaconf --quiet\n",
        "! pip install git+https://github.com/albumentations-team/albumentations --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWrNtNnHHCD4"
      },
      "source": [
        "# use wandb to track experiments : Comment this if not using wandb logger\n",
        "! wandb login 'a74f67fd5fae293e301ea8b6710ee0241f595a63'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjEzZVUaw897"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7i0wAgDxA51"
      },
      "source": [
        "# Grab the Data\n",
        "! unzip -qq /content/drive/My\\ Drive/Pascal\\ 2007\\ Data/pascal_voc_2007_test.zip\n",
        "! unzip -qq /content/drive/My\\ Drive/Pascal\\ 2007\\ Data/pascal_voc_2007_train_val.zip\n",
        "# Clone the RetinaNet Repo\n",
        "! git clone https://github.com/benihime91/pytorch_retinanet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD12esK_xP1K"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.chdir(\"/content/pytorch_retinanet\")\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-MtppNkzjPe"
      },
      "source": [
        "## **Data-Set Visulaization** :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44qIjriixflK"
      },
      "source": [
        "# Utilities to load in the Pascal dataset\n",
        "import pandas as pd\n",
        "from utils.pascal import get_pascal, generate_pascal_category_names\n",
        "from utils.pascal.pascal_transforms import compose_transforms\n",
        "import albumentations as A\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# compute transoformations\n",
        "tfms = compose_transforms()\n",
        "\n",
        "# paths to the voc dataset images and annotations\n",
        "test_ann_pth = \"/content/pascal_voc_2007_test/Annotations/\"\n",
        "test_im_pth = \"/content/pascal_voc_2007_test/Images/\"\n",
        "\n",
        "train_ann_pth = \"/content/pascal_voc_2007_train_val/Annotations/\"\n",
        "train_im_pth = \"/content/pascal_voc_2007_train_val/Images/\"\n",
        "\n",
        "# generate csv files for the train and test datasets\n",
        "trn_ds  = get_pascal(train_ann_pth, train_im_pth, \"train\", transforms=tfms)\n",
        "test_ds = get_pascal(test_ann_pth, test_im_pth,  \"test\",  transforms=tfms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_YkstkExrnq"
      },
      "source": [
        "df = pd.read_csv(\"pascal_train.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRXOIHwXyFY8"
      },
      "source": [
        "PASCAL_INSTANCE_CATEGORY_NAMES = generate_pascal_category_names(df)\n",
        "PASCAL_INSTANCE_CATEGORY_NAMES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz-1bG-3yzzh"
      },
      "source": [
        "from utils import visualize_boxes_and_labels_on_image_array\n",
        "from utils import collate_fn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dl = DataLoader(trn_ds, collate_fn=collate_fn, batch_size=5)\n",
        "bs = next(iter(dl)) # grab one batch\n",
        "image, target, idx = bs # unpack batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFAMTxBz3QEc"
      },
      "source": [
        "im = visualize_boxes_and_labels_on_image_array(\n",
        "    image=image[1].permute(1, 2, 0).numpy(),\n",
        "    boxes=target[1]['boxes'].numpy(),\n",
        "    scores=None,\n",
        "    classes=target[1]['labels'].numpy(),\n",
        "    label_map=PASCAL_INSTANCE_CATEGORY_NAMES,\n",
        ")\n",
        "\n",
        "im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGbi6bTzTKR"
      },
      "source": [
        "im = visualize_boxes_and_labels_on_image_array(\n",
        "    image=image[3].permute(1, 2, 0).numpy(),\n",
        "    boxes=target[3]['boxes'].numpy(),\n",
        "    scores=None,\n",
        "    classes=target[3]['labels'].numpy(),\n",
        "    label_map=PASCAL_INSTANCE_CATEGORY_NAMES,\n",
        ")\n",
        "\n",
        "im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ7bjvvAzely"
      },
      "source": [
        "## **Training the RetinaNet model** :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmstLYj5z4W6"
      },
      "source": [
        "# directory contents\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNlh0_8lzr_h"
      },
      "source": [
        "from omegaconf import OmegaConf, DictConfig\n",
        "import time\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import *\n",
        "from pytorch_lightning.callbacks import *\n",
        "\n",
        "from model import RetinaNetModel, LogCallback\n",
        "\n",
        "# seed so that results are reproducible\n",
        "pl.seed_everything(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-LRcxooz9W-"
      },
      "source": [
        "# ========================================================================= #\n",
        "# MODIFICATION OF THE CONFIG FILE TO FIX PATHS AND DATSET-ARGUEMENTS :\n",
        "# ========================================================================= #\n",
        "# Paths to the Images and the Annotations\n",
        "test_ann_pth  = \"/content/pascal_voc_2007_test/Annotations/\"\n",
        "test_im_pth   = \"/content/pascal_voc_2007_test/Images/\"\n",
        "\n",
        "train_ann_pth = \"/content/pascal_voc_2007_train_val/Annotations/\"\n",
        "train_im_pth  = \"/content/pascal_voc_2007_train_val/Images/\"\n",
        "\n",
        "# Paths to the hparam file for LightningModule\n",
        "hparams       = OmegaConf.load(\"/content/pytorch_retinanet/hparams.yaml\")\n",
        "\n",
        "# modify the haparams file\n",
        "# pascal 2007 dataset has 20 classes excluding the \"__background__\" class\n",
        "hparams.model.num_classes = 20 \n",
        "hparams.dataset.kind = \"pascal\"\n",
        "# Paths to the train and validation/Testing Datasets\n",
        "hparams.dataset.trn_paths = [train_ann_pth, train_im_pth]\n",
        "hparams.dataset.valid_paths = [test_ann_pth, test_im_pth]\n",
        "\n",
        "print(OmegaConf.to_yaml(hparams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIDSEwkp0UiK"
      },
      "source": [
        "# Instantie lightning-module\n",
        "litModel = RetinaNetModel(hparams=hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5lZ9lbNI5Ow"
      },
      "source": [
        "**Lightning Trainer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu6uttXD1NlZ"
      },
      "source": [
        "# ============================================================ #\n",
        "# INSTANTIATE LIGHTNING-TRAINER with CALLBACKS :\n",
        "# ============================================================ #\n",
        "# NOTE: \n",
        "# For a list of whole trainer specific arguments see : \n",
        "# https://pytorch-lightning.readthedocs.io/en/latest/trainer.html\n",
        "\n",
        "# Wandb logger\n",
        "# can use any other logger\n",
        "wb_name = f\"[{time.strftime('%m/%d %H:%M:%S')}]\"\n",
        "wb_p = \"pascal-2007\" \n",
        "wb_logger = WandbLogger(name=wb_name, project=wb_p, anonymous=\"allow\",)\n",
        "\n",
        "# Learning-rate Logger\n",
        "lr_logger = LearningRateLogger(logging_interval=\"step\")\n",
        "\n",
        "# Model Checkpoint\n",
        "fname =f\"/content/drive/My Drive/pascal_checkpoints/weights_pascal/\"\n",
        "os.makedirs(fname, exist_ok=True)\n",
        "checkpoint_callback = ModelCheckpoint(fname, mode=\"min\", monitor=\"val_loss\", save_top_k=3,)\n",
        "\n",
        "# callback for early-stopping\n",
        "early_stop_callback = EarlyStopping(mode=\"min\", monitor=\"val_loss\", patience=10,)\n",
        "\n",
        "trainer = Trainer(precision=16, \n",
        "                  num_sanity_val_steps=0,\n",
        "                  gpus=1, \n",
        "                  logger=[wb_logger],\n",
        "                  early_stop_callback=early_stop_callback, \n",
        "                  checkpoint_callback=checkpoint_callback,\n",
        "                  callbacks=[LogCallback(), lr_logger], \n",
        "                  weights_summary=None,\n",
        "                  terminate_on_nan = True, \n",
        "                  deterministic=True,\n",
        "                  max_epochs=55,\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyvMOPvu4FgZ"
      },
      "source": [
        "trainer.fit(litModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5rJskGFA9qN"
      },
      "source": [
        "## **Evaluating the trained-model** : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrha_GWi4HCN"
      },
      "source": [
        "# Evaluations results on the test/ validation dataset(if test dataset is not given)\n",
        "# using COCO API\n",
        "trainer.test(litModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcmsVktE4XtC"
      },
      "source": [
        "## **Saving the model weights** :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWWz3xSeBZEb"
      },
      "source": [
        "import torch\n",
        "\n",
        "path = f\"/content/drive/My Drive/pascal_weights_{int(time.time())}.pth\"\n",
        "torch.save(litModel.model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Jlr8ZNGuqW"
      },
      "source": [
        "## **Loading model weights :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95lTR3rQBfeo"
      },
      "source": [
        "from retinanet import Retinanet\n",
        "\n",
        "state_dict = torch.load(path)\n",
        "\n",
        "MODEL = Retinanet(num_classes=20, backbone_kind=\"resnet50\")\n",
        "MODEL.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBCuWQO2BbTO"
      },
      "source": [
        "## **Generating Predictions from the Model :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIOPikmB4FyN"
      },
      "source": [
        "# These are our classes\n",
        "PASCAL_INSTANCE_CATEGORY_NAMES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIwLxqKcEBlT"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from utils import visualize_boxes_and_labels_on_image_array\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_preds(path):\n",
        "    \"\"\"\n",
        "    Generates predictions on the given image from the given path.\n",
        "    \"\"\"\n",
        "    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    INFER_TRANSFORMS = A.Compose([A.ToFloat(max_value=255.0, always_apply=True),\n",
        "                                  ToTensorV2(always_apply=True)\n",
        "                                  ])\n",
        "    \n",
        "    TENSOR_IMAGE = INFER_TRANSFORMS(image=image)[\"image\"]\n",
        "    PREDICTIONS = MODEL.predict([TENSOR_IMAGE])\n",
        "    return PREDICTIONS[0]\n",
        "\n",
        "def filter_preds(ps, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Filters the predictions using given threshold.\n",
        "    \"\"\"\n",
        "    scores = ps[\"scores\"]\n",
        "    labels = ps[\"labels\"]\n",
        "    boxes = ps[\"boxes\"]\n",
        "\n",
        "    mask = scores > threshold\n",
        "\n",
        "    scores = scores[mask]\n",
        "    labels = labels[mask]\n",
        "    boxes = boxes[mask]\n",
        "    return scores.numpy(), labels.numpy(), boxes.numpy()\n",
        "\n",
        "\n",
        "def detect(image_path, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Generate detections on the image that is present in \n",
        "    the given image path\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the input Image\n",
        "        threshold: Score threshold to filter predictions\n",
        "\n",
        "    Returns: a PIL image containg the original Image and\n",
        "             bounding boxes draw over it.\n",
        "    \"\"\"\n",
        "    \n",
        "    # visualize_boxes_and_labels_on_image_array function\n",
        "    # expects the pixels values of the image to be in \n",
        "    # range [0,1] so be divide the loaded image by 255.0\n",
        "    # to noramlize the co-ordinates\n",
        "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB) / 255.0\n",
        "    \n",
        "    # Generate predictions for the given image\n",
        "    preds = get_preds(image_path)\n",
        "    # Filter predictions\n",
        "    scores, labels, boxes = filter_preds(preds, threshold)\n",
        "    # Draw all the bounding boxes over the Image\n",
        "    im = visualize_boxes_and_labels_on_image_array(\n",
        "        image,\n",
        "        boxes,\n",
        "        labels,\n",
        "        scores,\n",
        "        PASCAL_INSTANCE_CATEGORY_NAMES)\n",
        "    \n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hjH5gh08c_9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}