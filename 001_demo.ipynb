{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benihime91/pytorch_retinanet/blob/master/001_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_y280Hh3Bct"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "In this notebook, we implement [PyTorch RetinaNet](https://github.com/benihime91) for custom dataset. \n",
        "\n",
        "We will take the following steps to implement PyTorch RetinaNet on our custom data:\n",
        "* Install PyTorch RetinaNet along with required dependencies.\n",
        "* Download Custom Dataset.\n",
        "* Write Training Configuation yaml file .\n",
        "* Train  Detection Model .\n",
        "* Use Trained PyTorch RetinaNet Object Detection For Inference on Test Images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKhftVgvIZUC"
      },
      "source": [
        "### **Setting up Colab :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeiJgg30wyo6"
      },
      "source": [
        "# What GPU do we have ?\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwbGnBBawyiS"
      },
      "source": [
        "# Ensure colab doesn't disconnect\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d_3kuuO33HJ"
      },
      "source": [
        "# **Install Pytorch Retinanet and Dependencies** :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmBDcHdd6ZcP"
      },
      "source": [
        "# Clone the RetinaNet Repo\n",
        "!git clone https://github.com/benihime91/pytorch_retinanet.git\n",
        "# install dependencies\n",
        "!pip install pytorch-lightning omegaconf --quiet\n",
        "!pip install git+https://github.com/albumentations-team/albumentations --quiet\n",
        "!echo \"[   OK   ] Installed all depedencies \"\n",
        "\n",
        "#Update sys path to enclude the pytorch retinaet modules\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sys.path.append(\"/content/pytorch_retinanet/\")\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "!echo \"[   OK   ] Setup Done \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU1gg2tQ4JwT"
      },
      "source": [
        "# **Prepare Pytorch Retinanet Object Detection Training Data**\n",
        "\n",
        "We will use the **[Udacity Self Driving Car Dataset](https://public.roboflow.com/object-detection/self-driving-car)** from RoboFlow. The dataset contains 97,942 labels across 11 classes and 15,000 images\n",
        "\n",
        "\n",
        "To train on the custom dataset the data needs to be in either **csv** or **pascal-voc** format . Roboflow makes it easier to generate the datasets. We can directly download the datsets in required format.\n",
        "\n",
        "We will download the dataset in **Pascal-VOC** format and then use in-built methods available in PyTorch Retinanet to convert our data into **csv** format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccVmU0ii5qOk"
      },
      "source": [
        "#Downloading data from Roboflow\n",
        "#UPDATE THIS LINK - get our data from Roboflow\n",
        "%cd /content\n",
        "#curl -L \"[YOUR LINK HERE]\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "!curl -L \"https://public.roboflow.com/ds/C3CTTjCelU?key=G3KeQPl4TO\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9151YOX46gUg"
      },
      "source": [
        "#Set up paths \n",
        "\n",
        "#Path to where the Images are stored\n",
        "IMAGE_PATH = \"/content/export\"\n",
        "#Path to where annotations are stored\n",
        "ANNOT_PATH = \"/content/export\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44qIjriixflK"
      },
      "source": [
        "import pandas as pd\n",
        "from utils.pascal import convert_annotations_to_df\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5sQYcci76ad"
      },
      "source": [
        "## **Generate csv file from XML Annotations:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zNGhr6D7xGN"
      },
      "source": [
        "#convert xml files to pandas DataFrames\n",
        "train_df = convert_annotations_to_df(ANNOT_PATH, IMAGE_PATH, image_set=\"train\")\n",
        "!echo \"[   DONE  ] DataFrame(s) Generated !\"\n",
        "\n",
        "\n",
        "def remove_invalid_annots(df):\n",
        "    \"\"\"\n",
        "    Removes annotaitons where xmax, ymax < xmin,ymin\n",
        "    from the given dataframe\n",
        "    \"\"\"\n",
        "    df = df[df.xmax > df.xmin]\n",
        "    df = df[df.ymax > df.ymin]\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "    return df\n",
        "\n",
        "# removing annotations that are not valid annotations\n",
        "!echo \"[   INFO  ] Removing invalid annotations ... \"\n",
        "train_df = remove_invalid_annots(train_df)\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAvGbqvb1mdK"
      },
      "source": [
        "#We need to split the dataset in train, validation and test datasets\n",
        "def split_data(df1, size=0.3):\n",
        "    \"\"\"\n",
        "    Splits the given data according to\n",
        "    unique image ids into two dataframes\n",
        "    train and test . \n",
        "\n",
        "    Args:\n",
        "        df1  : dataframe to be split\n",
        "        size : size of the test set, train set will be of size (1-size) \n",
        "    \"\"\"\n",
        "    _ids = df1.filename.unique()\n",
        "    _trn_ids, _test_ids = train_test_split(_ids, test_size=size)\n",
        "    \n",
        "    df1[\"split\"] = 0\n",
        "\n",
        "    for i,idx in enumerate(df1.filename.values):\n",
        "        \n",
        "        if idx in set(_trn_ids)  : \n",
        "            df1[\"split\"][i] = \"train\"\n",
        "\n",
        "        elif idx in set(_test_ids) : \n",
        "            df1[\"split\"][i] = \"test\"\n",
        "\n",
        "    _trn, _test = df1.loc[df1[\"split\"] == \"train\"], df1.loc[df1[\"split\"] == \"test\"]\n",
        "    _trn, _test = _trn.reset_index(drop=True), _test.reset_index(drop=True)\n",
        "    return _trn, _test\n",
        "\n",
        "\n",
        "\n",
        "train_df, test_df = split_data(train_df, size=0.3)\n",
        "test_df, val_df   = split_data(test_df, size=0.5)\n",
        "\n",
        "\n",
        "!echo \"[   INFO  ] Num training images : {len(train_df.filename.unique())}\"\n",
        "!echo \"[   INFO  ] Num validation images : {len(val_df.filename.unique())}\"\n",
        "!echo \"[   INFO  ] Num test images : {len(test_df.filename.unique())}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GtBhU7OSXfN"
      },
      "source": [
        "### **CSV Files are as follows :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTqtATam6pQ1"
      },
      "source": [
        "train_df.drop(columns=[\"split\"], inplace=True)\n",
        "test_df.drop(columns=[\"split\"], inplace=True)\n",
        "val_df.drop(columns=[\"split\"], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfFsOeXvyNNI"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNshyfK6SPD2"
      },
      "source": [
        "val_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJYAKrj_SOnY"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1LWrVU72xLV"
      },
      "source": [
        "#Paths where to save the generated dataframes\n",
        "TRAIN_CSV = \"/content/train_data.csv\"\n",
        "VALID_CSV = \"/content/valid_data.csv\"\n",
        "TEST_CSV = \"/content/test_data.csv\"\n",
        "\n",
        "#Save the dataframes to memory\n",
        "train_df.to_csv(TRAIN_CSV, index=False)\n",
        "val_df.to_csv(VALID_CSV, index=False)\n",
        "test_df.to_csv(TEST_CSV, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiyBeJVe_Ofo"
      },
      "source": [
        "# **View Images from the Dataset** :\n",
        "\n",
        "We can use the fn `visualize_boxes_and_labels_on_image_array` from the RetinaNet repo to visualize images and the bounding boxes over them. To use this function we need to first create a Label Map, which is a list that contains all the classes at index corresponding to the integer labels ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_YkstkExrnq"
      },
      "source": [
        "##**Let's now generate the Label Map which is used for visualization:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRXOIHwXyFY8"
      },
      "source": [
        "from utils.pascal import generate_pascal_category_names\n",
        "\n",
        "LABEL_MAP = generate_pascal_category_names(train_df)\n",
        "LABEL_MAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8zlT8I_3Ey"
      },
      "source": [
        "##**Plot images with Bounding boxes over them**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz-1bG-3yzzh"
      },
      "source": [
        "from utils import visualize_boxes_and_labels_on_image_array\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlWOPTZz_-fx"
      },
      "source": [
        "def grab_bbs_(dataframe, index:int):\n",
        "    \"\"\"\n",
        "    Takes in a Pandas DataFrame and a index number\n",
        "    Returns filename of the image and all the bounding boxes and class_labels\n",
        "    corresponding the image that is at the given index\n",
        "    \"\"\"\n",
        "    assert index <= len(dataframe), f\"[  ERROR  ] Invalid index for dataframe with len: {len(dataframe)}\"\n",
        "    fname = dataframe.filename[index]\n",
        "    locs = dataframe.loc[dataframe.filename == fname]\n",
        "    bbs  = locs[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n",
        "    cls  = locs[\"labels\"].values\n",
        "    return fname, bbs, cls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mj_TbREDGio"
      },
      "source": [
        "### **Image from Train Data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g7KiJXfAfwU"
      },
      "source": [
        "#grab image, boxe and class target\n",
        "image, boxes, clas = grab_bbs_(train_df, index=0)\n",
        "\n",
        "#load and normalize the image\n",
        "image = Image.open(image)\n",
        "image = np.array(image) / 255.\n",
        "\n",
        "#draw boxes over the image\n",
        "image = visualize_boxes_and_labels_on_image_array(\n",
        "                image=image,\n",
        "                boxes=boxes, \n",
        "                scores=None, \n",
        "                classes=clas,\n",
        "                label_map=LABEL_MAP,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfQbwjKnDKVk"
      },
      "source": [
        "### **Image from Validation data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLn5jQirCOsz"
      },
      "source": [
        "#grab image, boxe and class target\n",
        "image, boxes, clas = grab_bbs_(val_df, index=50)\n",
        "\n",
        "#load and normalize the image\n",
        "image = Image.open(image)\n",
        "image = np.array(image) / 255.\n",
        "\n",
        "#draw boxes over the image\n",
        "image = visualize_boxes_and_labels_on_image_array(\n",
        "                image=image,\n",
        "                boxes=boxes, \n",
        "                scores=None, \n",
        "                classes=clas,\n",
        "                label_map=LABEL_MAP,\n",
        ")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dinrNPvFDN_z"
      },
      "source": [
        "###**Image from Test Data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFAMTxBz3QEc"
      },
      "source": [
        "#grab image, boxe and class target\n",
        "image, boxes, clas = grab_bbs_(test_df, index=100)\n",
        "\n",
        "#load and normalize the image\n",
        "image = Image.open(image)\n",
        "image = np.array(image) / 255.\n",
        "\n",
        "#draw boxes over the image\n",
        "image = visualize_boxes_and_labels_on_image_array(\n",
        "                image=image,\n",
        "                boxes=boxes, \n",
        "                scores=None, \n",
        "                classes=clas,\n",
        "                label_map=LABEL_MAP,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ7bjvvAzely"
      },
      "source": [
        "#**Configure Custom PyTorch RetianNet Object Detection Training Configuration** :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNlh0_8lzr_h"
      },
      "source": [
        "#let's look at the config file\n",
        "!cat /content/pytorch_retinanet/hparams.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-LRcxooz9W-"
      },
      "source": [
        "from omegaconf import OmegaConf\n",
        "\n",
        "#load in the hparams.ymal file using Omegaconf\n",
        "hparams = OmegaConf.load(\"/content/pytorch_retinanet/hparams.yaml\")\n",
        "!echo \"[  WORKING ] Writing Custom Configuration File .. \"\n",
        "# ========================================================================= #\n",
        "# MODIFICATION OF THE CONFIG FILE TO FIX PATHS AND DATSET-ARGUEMENTS :\n",
        "# ========================================================================= #\n",
        "#specify kind of data to use\n",
        "hparams.dataset.kind = \"csv\"\n",
        "#Paths to the csv files\n",
        "hparams.dataset.trn_paths   = TRAIN_CSV\n",
        "hparams.dataset.valid_paths = VALID_CSV\n",
        "hparams.dataset.test_paths  = TEST_CSV\n",
        "# number of classes in dataset excluding the \n",
        "# \"__background__\" class\n",
        "hparams.model.num_classes = len(LABEL_MAP) - 1\n",
        "#Changing optimizer paramters, \n",
        "#Scheduler can also be Optimized in the same way\n",
        "hparams.optimizer = {\n",
        "                \"class_name\": \"torch.optim.SGD\", \n",
        "                \"params\": {\n",
        "                    \"lr\": 8e-03,\n",
        "                    \"momentum\": 0.9,\n",
        "                    \"weight_decay\" : 0.001,\n",
        "                    }\n",
        "            }\n",
        "hparams.scheduler = {\n",
        "    \"class_name\": \"torch.optim.lr_scheduler.MultiStepLR\",\n",
        "    \"params\": {\n",
        "        \"milestones\" : [32, 42],\n",
        "        \"gamma\" : 0.1,\n",
        "    }\n",
        "}\n",
        "\n",
        "# adding augmentations\n",
        "hparams.transforms.append(\n",
        "    {\n",
        "        \"class_name\": \"albumentations.ShiftScaleRotate\",\n",
        "        \"params\": {\"p\": 0.5},\n",
        "        \"interval\": \"epoch\",\n",
        "        \"monitor\": False,\n",
        "    }\n",
        ")\n",
        "\n",
        "!echo \"[   DONE   ] Configuration File : \"\n",
        "print(OmegaConf.to_yaml(hparams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKDC6FqWFcPk"
      },
      "source": [
        "#**Instantiate Lightning-Module and Lightning-Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t27TtCaAFyWG"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import *\n",
        "from pytorch_lightning.callbacks import *\n",
        "from model import RetinaNetModel, LogCallback\n",
        "\n",
        "# seed so that results are reproducible\n",
        "pl.seed_everything(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HEM4pVdF8EC"
      },
      "source": [
        "##**Load in the Lighning-Module using the hparams file modified above :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIDSEwkp0UiK"
      },
      "source": [
        "# Instantie lightning-module\n",
        "litModel = RetinaNetModel(hparams=hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5lZ9lbNI5Ow"
      },
      "source": [
        "## **Load in the Lighning-Trainer :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu6uttXD1NlZ"
      },
      "source": [
        "# ============================================================ #\n",
        "# INSTANTIATE LIGHTNING-TRAINER with CALLBACKS :\n",
        "# ============================================================ #\n",
        "# NOTE: \n",
        "# For a list of whole trainer specific arguments see : \n",
        "# https://pytorch-lightning.readthedocs.io/en/latest/trainer.html\n",
        "\n",
        "# Wandb logger\n",
        "# can use any other logger\n",
        "save_dir = \"/content/logs\"\n",
        "LOGGER = TensorBoardLogger(save_dir=\"/content/logs\")\n",
        "# Learning-rate Logger\n",
        "LR_LOGGER = LearningRateLogger(logging_interval=\"step\")\n",
        "\n",
        "# Model Checkpoint\n",
        "fname =f\"/content/checkpoints/\"\n",
        "os.makedirs(fname, exist_ok=True)\n",
        "CHECKPOINT_CALLBACK = ModelCheckpoint(fname, mode=\"min\", monitor=\"val_loss\", save_top_k=3,)\n",
        "\n",
        "# callback for early-stopping\n",
        "EARLY_STOPPING_CALLBACK = EarlyStopping(mode=\"min\", monitor=\"val_loss\", patience=10,)\n",
        "\n",
        "# instantiate trainer with arguments from above\n",
        "trainer = Trainer(precision=16, \n",
        "                  max_epochs=50,\n",
        "                  num_sanity_val_steps=0,\n",
        "                  gpus=1, \n",
        "                  logger=[LOGGER],\n",
        "                  early_stop_callback=EARLY_STOPPING_CALLBACK, \n",
        "                  checkpoint_callback=CHECKPOINT_CALLBACK,\n",
        "                  callbacks=[LogCallback(), LR_LOGGER], \n",
        "                  weights_summary=None,\n",
        "                  terminate_on_nan=True, \n",
        "                  benchmark=True,\n",
        "                  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw0jpNl3GonQ"
      },
      "source": [
        "# **Train Custom Pytorch Retinanet Object Detector**:\n",
        "\n",
        "To train using pytorch-lightning we just need to run `trainer.fit(litModel)`.\n",
        "\n",
        "Properties (like data, optimzier, scheduler) are already defined in the `litModel` which was loaded using the custom modified `hparams` file.\n",
        "\n",
        "\n",
        "While `trainer` handles properties like Callbacks, fp_16 training, GPU training ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyvMOPvu4FgZ"
      },
      "source": [
        "!echo \"[   START   ] START TRAINING ... \"\n",
        "trainer.fit(litModel)\n",
        "!echo \"[    END    ] TRAINING COMPLETE ! \"\n",
        "!echo \"[    INFO   ] RUN  : %tensorboard --logdir {save_dir} to view TensorBoard Logs \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ__JAWIRe_V"
      },
      "source": [
        "### **Open TensorBoard Logs :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOoaUYSiRicG"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {save_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5rJskGFA9qN"
      },
      "source": [
        "#**Evaluating the trained-model using COCO-API Metrics** : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrha_GWi4HCN"
      },
      "source": [
        "# Evaluations results on the test/ validation dataset(if test dataset is not given)\n",
        "# using COCO API\n",
        "!echo \"[   START   ] START EVALUATION OF MODEL ON TEST IMAGE USING COCO-API ... \"\n",
        "trainer.test(litModel, ckpt_path=\"checkpoints/epoch=26.ckpt\")\n",
        "!echo \"[    END    ] DONE EVALUATING MODEL ON TEST IMAGE USING COCO-API ! \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcmsVktE4XtC"
      },
      "source": [
        "# **Export the model weights** :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWWz3xSeBZEb"
      },
      "source": [
        "import torch\n",
        "\n",
        "PATH = f\"/content/trained_weights.pth\"\n",
        "torch.save(litModel.model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Jlr8ZNGuqW"
      },
      "source": [
        "#**Load PyTorch Model from the trained Lightning-Module weights :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95lTR3rQBfeo"
      },
      "source": [
        "from retinanet import Retinanet\n",
        "\n",
        "state_dict = torch.load(PATH)\n",
        "\n",
        "MODEL = Retinanet(num_classes=hparams.model.num_classes, backbone_kind=hparams.model.backbone_kind)\n",
        "MODEL.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBCuWQO2BbTO"
      },
      "source": [
        "# **Run Inference on Test Images with Custom PyTroch Object Detector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIwLxqKcEBlT"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from utils import visualize_boxes_and_labels_on_image_array\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_preds(path, threshold=0.6,):\n",
        "    \"\"\"\n",
        "    Generates predictions on the given image from the given path.\n",
        "    \"\"\"\n",
        "    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    INFER_TRANSFORMS = A.Compose([A.ToFloat(max_value=255.0, always_apply=True),\n",
        "                                  ToTensorV2(always_apply=True)\n",
        "                                  ])\n",
        "    \n",
        "    TENSOR_IMAGE = INFER_TRANSFORMS(image=image)[\"image\"]\n",
        "    PREDICTIONS = MODEL.predict([TENSOR_IMAGE])\n",
        "    #print(PREDICTIONS[0])\n",
        "    return PREDICTIONS[0]\n",
        "\n",
        "def detect(image_path, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Generate detections on the image that is present in \n",
        "    the given image path\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the input Image\n",
        "        threshold: Score threshold to filter predictions\n",
        "        nms_threshold: NMS threshold\n",
        "\n",
        "    Returns: a PIL image containg the original Image and\n",
        "             bounding boxes draw over it.\n",
        "    \"\"\"\n",
        "    \n",
        "    # visualize_boxes_and_labels_on_image_array function\n",
        "    # expects the pixels values of the image to be in \n",
        "    # range [0,1] so be divide the loaded image by 255.0\n",
        "    # to noramlize the co-ordinates\n",
        "    # load the image as numpy array\n",
        "    image = Image.open(image_path)\n",
        "    image = np.array(image) / 255.\n",
        "    # Generate predictions for the given image\n",
        "    preds = get_preds(image_path, threshold,)\n",
        "    # print(preds)\n",
        "    # Filter predictions\n",
        "    boxes, labels, scores = preds[\"boxes\"], preds[\"labels\"], preds[\"scores\"]\n",
        "    mask = scores > threshold\n",
        "    boxes = boxes[mask]\n",
        "    labels = labels[mask]\n",
        "    scores = scores[mask]\n",
        "    return boxes.numpy(), labels.numpy(), scores.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hjH5gh08c_9"
      },
      "source": [
        "IMAGE, REAL_BOXES, REAL_LABELS = grab_bbs_(test_df, index=4)\n",
        "IMAGE = Image.open(IMAGE)\n",
        "IMAGE = np.array(IMAGE) / 255.\n",
        "\n",
        "#draw boxes over the image\n",
        "REAL_IMAGE = visualize_boxes_and_labels_on_image_array(\n",
        "                image=IMAGE,\n",
        "                boxes=REAL_BOXES, \n",
        "                scores=None, \n",
        "                classes=REAL_LABELS,\n",
        "                label_map=LABEL_MAP,\n",
        ")\n",
        "\n",
        "REAL_IMAGE\n",
        "\n",
        "PATH = test_df.filename[4]\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "PRED_BOXES, PRED_LABELS, PRED_SCORES = detect(PATH, THRESHOLD,)\n",
        "\n",
        "PRED_IMAGE = visualize_boxes_and_labels_on_image_array(\n",
        "                image=IMAGE, \n",
        "                boxes=PRED_BOXES, \n",
        "                scores=PRED_SCORES,\n",
        "                classes= PRED_LABELS,\n",
        "                label_map=LABEL_MAP,\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ21x_UtvODn"
      },
      "source": [
        "!echo \"[ INFERENCE ] ORIGINAL\"\n",
        "\n",
        "REAL_IMAGE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS3Z9sdUtntn"
      },
      "source": [
        "!echo \"[ INFERENCE ] PREDICTIONS \"\n",
        "\n",
        "PRED_IMAGE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ4bW5bpUaAz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}