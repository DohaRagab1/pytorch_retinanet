from typing import *

import albumentations as A
from albumentations.pytorch import ToTensorV2 as ToTensor

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------


# -----------------------------------------------------------------------------
# Input Options
# -----------------------------------------------------------------------------
MEAN: List[float] = [0.485, 0.456, 0.406]
# `Mean values` used for input normalization.
STD: List[float] = [0.229, 0.224, 0.225]
# `STD values` used for input normalization.
MIN_IMAGE_SIZE: int = 600
# `Minimum size` of the image to be rescaled before feeding it to the backbone
MAX_IMAGE_SIZE: int = 1333
# `Maximum` size of the image to be rescaled before feeding it to the backbone


# -----------------------------------------------------------------------------
# `Dataset` & `DataLoader` Options
# -----------------------------------------------------------------------------
# Csv File Options. Each Item in the csv should `correspond` to a single annotation.
# `targets` should be `Integers`.
# Path(s) to the csv `file` or the `csv` files.
TRAIN_CSV_DIR: str = None
VALID_CSV_DIR: str = None

# csv header pointing to the `image_paths`.
IMG_HEADER: str = 'filepath'

# csv header pointing to the `xmin` of `annotations`.
XMIN_HEADER: str = 'xmin'
# csv header pointing to the `ymin` of `annotations`.
YMIN_HEADER: str = 'ymin'
# csv header pointing to the `xmax` of `annotations`.
XMAX_HEADER: str = 'xmax'
# csv header pointing to the `ymax` of `annotations`.
YMAX_HEADER: str = 'ymax'
# csv header pointing to the `classes` of the `bboxes`.
CLASS_HEADER: str = 'targets'

# Albumentations transformations to apply to the `Images` & `bboxes`
# check : https://albumentations.ai/docs/getting_started/transforms_and_targets/
# to see which transformations are valid for bbox augmentations.

# Valid Transformations
VALID_TRANSFORMATIONS: List = [
    A.ToFloat(max_value=255., always_apply=True),
    ToTensor(always_apply=True)
]


# Train Transformations
TRAIN_TRANSFORMATIONS: List = [
    A.CLAHE(),
    A.RandomBrightness(),
    A.HueSaturationValue(),
    A.OneOf([
        A.RandomRain(),
        A.RandomFog(),
        A.RandomSunFlare(),
        A.RandomBrightnessContrast(),
        A.GaussianBlur()
    ]),
    A.IAASharpen(),
    A.HorizontalFlip(),
    A.Cutout(),
    A.ToGray(p=0.25)
]
TRAIN_TRANSFORMATIONS = TRAIN_TRANSFORMATIONS + VALID_TRANSFORMATIONS

TRANSFORMATIONS: Dict[str, A.Compose] = {
    'train_transforms': (
        A.Compose(
            TRAIN_TRANSFORMATIONS,
            p=1.0,
            bbox_params=A.BboxParams(
                format='pascal_voc',
                label_fields=['class_labels']
            ))),
    'valid_transforms': (
        A.Compose(
            VALID_TRANSFORMATIONS,
            p=1.0,
            bbox_params=A.BboxParams(
                format='pascal_voc',
                label_fields=['class_labels']
            ))),
}

# Options for a `PyTorch` `DataLoader` instance
SHUFFLE: bool = True
BATCH_SIZE: bool = 8
PIN_MEMORY: bool = True
NUM_WORKERS: int = 0
DROP_LAST: bool = False


# -----------------------------------------------------------------------------
# Anchor Generator Options
# -----------------------------------------------------------------------------
ANCHOR_SIZES: List[float] = [
    [x, x * 2**(1/3), x * 2**(2/3)] for x in [32, 64, 128, 256, 512]
]
# Anchor sizes (i.e. sqrt of area) in absolute pixels w.r.t. the network input.
ANCHOR_STRIDES: List[int] = [8, 16, 32, 64, 128]
# A list of float value representing the strides for each feature
# map in the feature pyramid.
ANCHOR_OFFSET: float = 0.
# Relative offset between the center of the first anchor and the top-left corner of the image
# Value has to be in [0, 1). Recommend to use 0.5, which means half stride.
# The value is not expected to affect model accuracy.
ANCHOR_ASPECT_RATIOS: List[float] = [0.5, 1.0, 2.0]
# Anchor aspect ratios. For each area given in `SIZES`, anchors with different aspect
# ratios are generated by an anchor generator.
IOU_THRESHOLDS_FOREGROUND: float = 0.4
IOU_THRESHOLDS_BACKGROUND: float = 0.5
# IoU overlap ratio `bg`, `fg` for labeling anchors.
IGNORE_IDX: Any[int] = -2
BACKGROUND_IDX: Any[int] = -1
# Anchors with < bg are labeled negative (-1)
# Anchors  with >= bg and < fg are ignored (-2)
BBOX_REG_WEIGHTS: List[float] = [1.0, 1.0, 1.0, 1.0]
# Weights on (dx, dy, dw, dh) for normalizing Retinanet anchor regression targets


# -----------------------------------------------------------------------------
# `RetinaNet` Options
# -----------------------------------------------------------------------------
NUM_CLASSES: int = 80
# number of output classes of the model(excluding the background).
# `-1` will always be the background class. Target values should have values starting from 0

# The network used to compute the features for the model.
# Should be one of ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet101', 'resnet152'].
BACKBONE: str = 'resnet50'

# Prior prob for rare case (i.e. foreground) at the beginning of training.
# This is used to set the bias for the logits layer of the classifier subnet.
# This improves training stability in the case of heavy class imbalance.
PRIOR: float = 0.01

# Wether the backbone should be pretrained or not,. If true loads `pre-trained`
# weights from `Imagenet`
PRETRAINED_BACKBONE: bool = True

# Overlap threshold used for non-maximum suppression (suppress boxes with
# IoU >= this threshold)
NMS_THRES: float = 0.5

# Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to
# balance obtaining high recall with not having too many low precision
# detections that will slow down inference post processing steps (like NMS)
# A default threshold of 0.0 increases AP by ~0.2-0.3 but significantly slows down
# inference.
SCORE_THRES: float = 0.05
MAX_DETECTIONS_PER_IMAGE: int = 500

# Wether to freeze `BatchNormalization` layers of `backbone`
FREEZE_BN: bool = True

# Loss parameters
FOCAL_LOSS_GAMMA: float = 2.0
FOCAL_LOSS_ALPHA: float = 0.25
